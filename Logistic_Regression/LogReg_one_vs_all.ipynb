{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-vs-All Classification\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [The Dataset](#dataset)\n",
    "- [The Regularized Cost Function and Gradient](#cost_and_gradient)\n",
    "- [Training the Models](#training)\n",
    "- [Making Predictions](#predictions)\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "In a [previous article](https://github.com/marty-vanhoof/Maching_Learning/blob/master/Logistic_Regression/Logistic_Regression.ipynb), we talked in detail about logistic regression in the case of binary classification; that is, given a set of training data $\\{ \\, ( \\mathbf{x}_1,y_1 ), ( \\mathbf{x}_2,y_2 ) \\ldots,  ( \\mathbf{x}_m,y_m ) \\, \\}$, logistic regression finds the parameters $\\theta_0, \\theta_1, \\ldots, \\theta_n$ of a function $h_\\theta(\\mathbf{x})$ that estimates the probability of a class label $y=1$ given a feature vector $\\mathbf{x}$, and so the function $1 - h_\\theta(\\mathbf{x})$ estimates the probability of a class label $y=0$ given $\\mathbf{x}$.  We can then predict binary outcomes by specifying a probability threshold, and the threshold we use is given as follows\n",
    "\n",
    "$$ \\mathrm{prediction} =\n",
    "\\begin{cases}\n",
    "1 & \\mathrm{if} \\quad P(y = 1 \\,|\\, \\mathbf{x} ; \\theta) \\geq 0.5 \\\\ \n",
    "0 & \\mathrm{if} \\quad P(y = 0 \\,|\\, \\mathbf{x} ; \\theta) < 0.5\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "One-vs-all classification extends logistic regression to the case of classifying more than two class labels by considering a sequence of binary classifications.  Suppose the target variable $y$ can be labeled in $k$ possible ways, so $y \\in \\{1, 2, \\ldots, k \\}$ with $k \\geq 2$.  We can turn this into a sequence of binary classification problems and use logistic regression at each step.  \n",
    "\n",
    "First separate the labels $\\{1, 2, \\dots, k \\}$ into either 1 or $\\{2, \\dots, k \\}$ and fit a standard logistic regression model to this.  This logistic regression model will find a function $h_\\theta^{(1)}(\\mathbf{x})$ that classifies the data into either 1 or $\\{ 2, \\ldots, k \\}$.\n",
    "\n",
    "Next, separate the labels into either 2 or $\\{ 1, 3, \\dots, k \\}$ and fit another logistic regression model to this, resulting in a function $h_\\theta^{(2)}(\\mathbf{x})$ that classifies the data into either 2 or $\\{ 1, 3, \\dots, k \\}$.\n",
    "And so on...the last step will fit another logistic regression model that finds a function $h_\\theta^{(k)}(\\mathbf{x})$ that classifies the data into either $k$ or $\\{ 1, 2, \\ldots, k-1 \\}$.\n",
    "\n",
    "The result is that we have $k$ classifiers $h_\\theta^{(1)}(\\mathbf{x}), h_\\theta^{(2)}(\\mathbf{x}), \\ldots, h_\\theta^{(k)}(\\mathbf{x})$ each trying to estimate the probability of a class label $y \\in \\{ 1, 2, \\ldots, k \\}$ given $\\mathbf{x}$.  That is,\n",
    "\n",
    "$$\n",
    "h_\\theta^{(i)}(\\mathbf{x}) = P(y = i \\,|\\, \\mathbf{x}; \\theta) \\,, \\quad i = 1,2, \\ldots, k\n",
    "$$\n",
    "\n",
    "Then to make a prediction on a feature vector $\\mathbf{x}$, pick the class $i$ that gives maximum probability\n",
    "\n",
    "$$\n",
    "\\underset{i}{\\mathrm{argmax}} \\, h_\\theta^{(i)}(\\mathbf{x}) \n",
    "$$\n",
    "\n",
    "A nice discussion of one-vs-all classification can be found in [this video](https://www.youtube.com/watch?v=BzSsQWhDRXE&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW).\n",
    "\n",
    "<a id='dataset'></a>\n",
    "### The Dataset\n",
    "As usual, we will be working through an example from one of the programming exercises in Andrew Ng's machine learning course.  We will implement one-vs-all logistic regression to recognize hand-written digits.  The [dataset](https://github.com/marty-vanhoof/Maching_Learning/blob/master/data/logReg_data3.mat) contains 5000 training examples of handwritten digits, which is a subset of the MNIST database of handwritten digits available [here](http://yann.lecun.com/exdb/mnist/).  This picture below is an example of some of the digits.\n",
    "<br/>\n",
    "<img src=\"digits.png\" height=\"400\" width=\"400\">\n",
    "<br/>\n",
    "Each training example (each little box) is a 20 pixel by 20 pixel grayscale image of the digit.  The image can be represented as a 400 dimensional vector $\\mathbf{x}_i$ of floating-point numbers, where each number represents the grayscale intensity of a pixel at that location.  Each of these training examples becomes a single row in a feature matrix $X$.  The class labels are in the target vector $\\mathbf{y} = (y_1, y_2, \\ldots, y_{400})^T$, where each $y_i$ can be one of the numbers 0 through 9\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\, - \\,\\, \\mathbf{x}_1^T - \\, \\\\\n",
    "\\, - \\,\\, \\mathbf{x}_2^T - \\, \\\\\n",
    "\\vdots \\\\\n",
    "\\, - \\,\\, \\mathbf{x}_{400}^T - \\,\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{400} \n",
    "\\end{bmatrix} \n",
    "$$ \n",
    "\n",
    "The dataset is in a MATLAB format, so we have to use a function `loadmat()` in the SciPy library in order for Pandas to interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " '__globals__': [],\n",
       " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ..., \n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.io import loadmat  \n",
    "%matplotlib inline\n",
    "\n",
    "filepath = os.getcwd() + '/logReg_data3.mat'\n",
    "data = loadmat('logReg_data3.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should make sure that $X$ and $\\mathbf{y}$ have the proper shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "#X, y = data['X'], data['y']\n",
    "print(data['X'].shape, data['y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='cost_and_gradient'></a>\n",
    "### The Regularized Cost Function and Gradient\n",
    "\n",
    "Since we are going to use one-vs-all logistic regression to classify the digits 0 through 9, we will need to train 10 separate logistic regression models.  We will re-use the logistic cost function and the gradient code from before, and we will also implement the regularized versions of these functions.  In matrix (vectorized) form, the regularized cost function is given as\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\big[\\, \\mathbf{y}^T \\mathbf{ln}_h + (\\mathbf{1} - \\mathbf{y})^T \\mathbf{ln}_{1 - h} \\,\\big] + \\frac{\\lambda}{2m} ||\\, \\widehat{\\theta} \\,||^2 \\,, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{1} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{ln}_h =\n",
    "\\begin{bmatrix}\n",
    "\\ln h_\\theta(\\mathbf{x}_1) \\\\\n",
    "\\ln h_\\theta(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\ \n",
    "\\ln h_\\theta(\\mathbf{x}_m)\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{ln}_{1-h} =\n",
    "\\begin{bmatrix}\n",
    "\\ln (1 - h_\\theta(\\mathbf{x}_1)) \\\\\n",
    "\\ln (1 - h_\\theta(\\mathbf{x}_2)) \\\\\n",
    "\\vdots \\\\ \n",
    "\\ln (1 - h_\\theta(\\mathbf{x}_m)) \n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\widehat{\\theta} =\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n\n",
    "\\end{bmatrix} \\,,\n",
    "$$\n",
    "\n",
    "\n",
    "and $m$ is the number of training examples.  The Python function `regularized_logReg_cost()` implements $J(\\theta)$ below.\n",
    "\n",
    "The gradient of the regularized cost function is the vector $\\nabla J(\\theta)$ whose components are given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\theta_0} &= \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) x_{ij} \\quad \\textrm{for } j=0 \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_j} &= \\frac{1}{m} \\bigg( \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) x_{ij} + \\lambda \\theta_j \\bigg) \\quad \\textrm{for } j \\geq 1\n",
    "\\end{align*}\n",
    "\n",
    "The computation of the gradient $\\nabla J(\\theta)$ is implemented in the function `regularized_logReg_gradient()` below in vectorized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from log_reg_scripts import logReg_cost, logistic\n",
    "\n",
    "def regularized_logReg_cost(theta, X, y, lambda_):\n",
    "    '''Compute the regularized cost function.  This is almost the same as\n",
    "    logReg_cost() from before, but now we're adding a regularization term.\n",
    "    '''\n",
    "    # re-use logReg_cost from before\n",
    "    unreg_cost = logReg_cost(theta, X, y)\n",
    "    # convert theta back to a numpy array since logReg_cost converts it to a numpy matrix\n",
    "    theta = np.array(theta)\n",
    "    reg_term = lambda_ / (2*len(y)) * sum(theta[1:]**2) \n",
    "    return unreg_cost + reg_term\n",
    "\n",
    "def regularized_logReg_gradient(theta, X, y, lambda_):\n",
    "    '''Compute the gradient of the regularized logistic cost function.\n",
    "    '''\n",
    "    \n",
    "    theta, X, y = np.matrix(theta), np.matrix(X), np.matrix(y)\n",
    "    \n",
    "    h_theta = logistic(X*theta.T)\n",
    "    gradient = ( 1 / len(y) )*(h_theta - y).T * X \n",
    "    theta_temp = theta\n",
    "    theta_temp[0, 0] = 0\n",
    "    reg_gradient = gradient + (lambda_ / len(y))*theta_temp\n",
    "    \n",
    "    return np.array(reg_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='training'></a>\n",
    "### Training the Models\n",
    "\n",
    "We are now going to train the 10 different classifiers using the function `one_vs_all()` below.  Standard logistic regression can only distinguish between two classes at a time, so given our labels $L = \\{0,1,2, \\ldots,9 \\}$, each logistic regression model will classify the data into \"class $i$\" or \"not class $i$\", where $i$ is a number in our labels set $L$. \n",
    "\n",
    "The function below will loop over each class label, and then find the optimal theta parameters for each label using the [minimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) function in SciPy's optimize module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def one_vs_all(X, y, num_labels, lambda_):\n",
    "    '''\n",
    "    Trains num_labels logisitic regression classifiers and returns each of these classifiers\n",
    "    in a matrix all_theta, where the i-th row of all_theta corresponds to the classifier \n",
    "    for label i\n",
    "    '''\n",
    "    \n",
    "    # m = number of training examples, n = number of features\n",
    "    m, n = X.shape[0], X.shape[1]\n",
    "    \n",
    "    # all_theta will be populated with all the parameters of our model\n",
    "    all_theta = np.zeros((num_labels, n+1))\n",
    "    \n",
    "    # insert column of ones into X\n",
    "    X = np.insert(X, 0, np.ones(m), axis=1)\n",
    "    \n",
    "    # for each label, train a separate classifier\n",
    "    # Note: the labels are 1-indexed instead of 0-indexed due MATLAB being 1-indexed\n",
    "    for i in range(1, num_labels+1):\n",
    "        theta = np.zeros(n+1)\n",
    "        y_i = (y == i).astype(int)\n",
    "        # use scipy.optimize's minimize function to minimize regularized_logReg_cost\n",
    "        # and store the resulting parameters in all_theta\n",
    "        result = minimize(fun=regularized_logReg_cost, x0=theta, args=(X, y_i, lambda_),\n",
    "                        method='TNC', jac=regularized_logReg_gradient)\n",
    "        all_theta[i-1, :] = result.x\n",
    "        \n",
    "    return all_theta\n",
    "\n",
    "all_theta = one_vs_all(data['X'], data['y'], 10, 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='predictions'></a>\n",
    "### Making Predictions\n",
    "\n",
    "Now we are ready to use our model to make predictions and then evaluate the accuracy on the training set.  Recall that the **accuracy** is defined as the number of correct predictions divided by the total number of predictions.  Of course, evaluating the accuracy only on the training set is a big mistake in machine learning because the ultimate goal is that we want the model to generalize to new unseen data.  The importance of splitting the data into training and testing sets will be discussed in a future write-up.\n",
    "\n",
    "Recall from our discussion at the beginning that after training all of the classifiers, we can make a prediction on a feature vector $\\mathbf{x}$ by picking the class that gives maximum probability\n",
    "\n",
    "$$\n",
    " \\underset{i}{\\mathrm{argmax}} \\, P(y = i \\,|\\, \\mathbf{x}; \\theta) \\,.\n",
    "$$\n",
    "\n",
    "The `predict()` function implements this below and then the `accuracy()` function computes the accuracy of the model on the training data.  We can see that on the training set, the percentage of correct predictions is 94.46%.  This is nice, but we have no idea if the model is overfitting since we didn't compare the accuracy on a test set.\n",
    "\n",
    "In the next article we will see how to use a neural network to classify handwritten digits using the same training set we used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(all_theta, X):\n",
    "    '''\n",
    "    Predict the label for each training example in X. Returns an array of m predictions.\n",
    "    \n",
    "    all_theta is a matrix (n-dim numpy array) where the i-th row is a trained logistic\n",
    "    regression theta vector for the i-th class.\n",
    "    X is the feature matrix with m rows (each row being a training example).\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    num_labels = all_theta.shape[0]\n",
    "    # insert column of ones into X\n",
    "    X = np.insert(X, 0, np.ones(m), axis=1)\n",
    "    # probs stores num_labels class probabilities for each training example\n",
    "    probs = logistic(X @ all_theta.T)\n",
    "    # pick the label with max probability for each training example\n",
    "    # Note: the labels are 1-indexed instead of 0-indexed due MATLAB being 1-indexed\n",
    "    predictions = np.argmax(probs, axis = 1) + 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def accuracy(predictions, y):\n",
    "    '''\n",
    "    Compute the accuracy of the model by comparing the\n",
    "    array of predictions to the target vector y.\n",
    "    Returns the proportion of correct predictions.\n",
    "    '''\n",
    "    y = y.reshape((5000,))\n",
    "    correct = (predictions == y).astype(int)\n",
    "    accuracy = np.sum(correct) / len(predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 94.46%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(all_theta, data['X'])\n",
    "accuracy_percent = accuracy(predictions, data['y']) * 100\n",
    "print('accuracy: {}%'.format(accuracy_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
