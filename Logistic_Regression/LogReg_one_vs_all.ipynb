{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-vs-All Classification\n",
    "\n",
    "In a [previous article](https://github.com/marty-vanhoof/Maching_Learning/blob/master/Logistic_Regression/Logistic_Regression.ipynb), we talked in detail about logistic regression in the case of binary classification; that is, given a set of training data $\\{ \\, ( \\mathbf{x}_1,y_1 ), ( \\mathbf{x}_2,y_2 ) \\ldots,  ( \\mathbf{x}_m,y_m ) \\, \\}$, logistic regression finds the parameters $\\theta_0, \\theta_1, \\ldots, \\theta_n$ of a function $h_\\theta(\\mathbf{x})$ that estimates the probability of a class label $y=1$ given a feature vector $\\mathbf{x}$.  The function $1 - h_\\theta(\\mathbf{x})$ estimates the probability of a class label $y=0$ given $\\mathbf{x}$.  We can then predict binary outcomes by specifying a probability threshold, and the threshold we use is given as follows\n",
    "\n",
    "$$ \\mathrm{prediction} =\n",
    "\\begin{cases}\n",
    "1 & \\mathrm{if} \\quad P(y = 1 \\,|\\, \\mathbf{x} ; \\theta) \\geq 0.5 \\\\ \n",
    "0 & \\mathrm{if} \\quad P(y = 0 \\,|\\, \\mathbf{x} ; \\theta) < 0.5\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "One-vs-all classification extends logistic regression to the case of classifying more than two class labels by considering a sequence of binary classifications.  Suppose the predictor variable $y$ can be labeled in $k$ possible ways, so $y \\in \\{1, 2, \\ldots, k \\}$ with $k \\geq 2$.  We can turn this into a sequence of binary classification problems and use logistic regression at each step.  \n",
    "\n",
    "First separate the labels $\\{1, 2, \\dots, k \\}$ into either 1 or $\\{2, \\dots, k \\}$ and fit a standard logistic regression model to this.  This logistic regression model will find a function $h_\\theta^{(1)}(\\mathbf{x})$ that classifies the data into either 1 or $\\{ 2, \\ldots, k \\}$.\n",
    "\n",
    "Next, separate the labels into either 2 or $\\{ 1, 3, \\dots, k \\}$ and fit another logistic regression model to this, resulting in a function $h_\\theta^{(2)}(\\mathbf{x})$ that classifies the data into either 2 or $\\{ 1, 3, \\dots, k \\}$.\n",
    "And so on...the last step will fit another logistic regression model that finds a function $h_\\theta^{(k)}(\\mathbf{x})$ that classifies the data into either $k$ or $\\{ 1, 2, \\ldots, k-1 \\}$.\n",
    "\n",
    "The result is that we have $k$ classifiers $h_\\theta^{(1)}(\\mathbf{x}), h_\\theta^{(2)}(\\mathbf{x}), \\ldots, h_\\theta^{(k)}(\\mathbf{x})$ each trying to estimate the probability of a class label $y \\in \\{ 1, 2, \\ldots, k \\}$ given $\\mathbf{x}$.  That is,\n",
    "\n",
    "$$\n",
    "h_\\theta^{(i)}(\\mathbf{x}) = P(y = i \\,|\\, \\mathbf{x}; \\theta) \\,, \\quad i = 1,2, \\ldots, k\n",
    "$$\n",
    "\n",
    "Then to make a prediction on a feature vector $\\mathbf{x}$, pick the class $i$ that gives maximum probability\n",
    "\n",
    "$$\n",
    "\\max \\limits_i h_\\theta^{(i)}(\\mathbf{x}) \n",
    "$$\n",
    "\n",
    "A nice discussion of one-vs-all classification can be found in [this video](https://www.youtube.com/watch?v=BzSsQWhDRXE&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " '__globals__': [],\n",
       " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ..., \n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.io import loadmat  \n",
    "%matplotlib inline\n",
    "\n",
    "filepath = os.getcwd() + '/logReg_data3.mat'\n",
    "data = loadmat('logReg_data3.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
