{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression (despite its name) is a supervised classification model.  Recall that in supervised learning we have training data that consists of a set of pairs $\\{ \\, ( \\mathbf{x}_1,y_1 ), ( \\mathbf{x}_2,y_2 ) \\ldots,  ( \\mathbf{x}_m,y_m ) \\, \\}$, where each $\\mathbf{x}_i$ is a feature vector and the target variable $y_i$ is the corresponding label.  In a classification problem, each $y_i$ takes values in a finite unordered set, and we want to find a function $h_\\theta$ that takes a feature vector $\\mathbf{x}_i$ and tries to predict the target variable $y_i$.  We want this function to generalize as well as possible to new data.  An example of such a function $h_\\theta$ would be a spam classifier that takes email data as input and tries to predict whether it is spam or not. \n",
    "\n",
    "Often we are interested in estimating the *probability* of a particular label, given some data. In **logistic regression**, we want to model the probability of a class label $y$ given a number of features $x_1, x_2, \\ldots, x_n$.  Let's start with the case where the target variable is binary, so $y$ can only take the values 0 or 1.  Given a feature vector $\\mathbf{x} = (x_1, \\ldots, x_n)^T$, our function $h_\\theta(\\mathbf{x})$ should estimate the probability that $y$ takes a specfied value in the set $ \\{ 0,1 \\} $.  For example, financial institutions are interested in predicting whether credit card transactions are fraudulent.  It is more valuable to have an estimate of the probability that a transaction is fraudulent, rather than a classification of fraudulent or not.  In such a model, 0 could represent 'not fraudulent' and 1 could represent 'fraudulent', and the function $h_\\theta$ would predict the probability that a credit card transaction is fraudulent based on some features $\\mathbf{x}$.\n",
    "\n",
    "So what is a good candidate for a function $h_\\theta$? We could try to mimic linear regression by attempting to find parameters $\\theta_0, \\theta_1, \\ldots, \\theta_n$ in order to fit a linear function of the form\n",
    "\n",
    "$$ h_\\theta(\\mathbf{x}) = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_n x_n \\,, $$\n",
    "\n",
    "but this is not ideal for a few reasons; one of the reasons being that some of the predicted probabilities would fall outside the interval [0,1].  For a discussion, see the text [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) - Section 4.2.  \n",
    "\n",
    "It turns out that a good model for these probabilities is the **logistic function**\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} \\,. $$\n",
    "\n",
    "when $z$ is written as a linear combination of the features  \n",
    "\n",
    "$$ z = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_n x_n = \\theta^T \\mathbf{x} \\,. $$  \n",
    "\n",
    "The logistic function takes values in the unit interval [0,1] and it satisfies\n",
    "\n",
    "$$ \\lim_{z \\rightarrow -\\infty} g(z) = 0 \\quad \\mathrm{and} \\quad \\lim_{z \\rightarrow \\infty} g(z) = 1 \\,. $$\n",
    "\n",
    "A graph of the logistic function is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6XFZ+tcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cc675f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def logistic(z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# plot the function\n",
    "x = np.arange(-10, 11)\n",
    "plt.plot(x, logistic(x), color = 'blue')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)', rotation = 0, labelpad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the function $h_\\theta$ ( the function we will use to make predictions ) as follows:\n",
    "\n",
    "$$ h_\\theta(\\mathbf{x}) = g(\\theta^T \\mathbf{x}) = \\frac{1}{1 + e^{ \\, -\\theta^T \\mathbf{x}} } \\,. $$\n",
    "\n",
    "This function $h_\\theta$ will model the probability of a class label $y = 1$ given the feature vector $\\mathbf{x}$, and parametrized by $\\theta$\n",
    "\n",
    "$$ P(y = 1 \\,|\\, \\mathbf{x} ; \\theta) = h_\\theta(\\mathbf{x}) \\,. $$\n",
    "\n",
    "Since there are only two classes ( 0 and 1 ), the probability that $y = 0$, given $\\mathbf{x}$ and parametrized by $\\theta$, can be written as\n",
    "\n",
    "$$ P(y = 0 \\,|\\, \\mathbf{x} ; \\theta) = 1 - h_\\theta(\\mathbf{x}) \\,. $$\n",
    "\n",
    "An important thing to note is that we are treating $\\mathbf{x}$ as a multivariate random variable and $\\theta$ as a vector of parameters that we are trying to estimate.  Observe that we can write\n",
    " \n",
    "$$ P(y \\,|\\, \\mathbf{x} ; \\theta) = h_\\theta(\\mathbf{x})^y (1 - h_\\theta(\\mathbf{x}))^{1-y} \\, \\qquad (1) $$\n",
    "\n",
    "since $y$ can only be one of the two values 0 or 1.  Now suppose that we have $m$ training examples $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_m$ that we put as the rows of a feature matrix $X$, and we also have the target vector $\\mathbf{y}$ that holds the $m$ labels\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\, - \\,\\, \\mathbf{x}_1^T - \\, \\\\\n",
    "\\, - \\,\\, \\mathbf{x}_2^T - \\, \\\\\n",
    "\\vdots \\\\\n",
    "\\, - \\,\\, \\mathbf{x}_m^T - \\,\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Using the matrix $X$ and the vector $\\mathbf{y}$, the probability of the data is written as \n",
    "\n",
    "$$ P(\\mathbf{y} \\,|\\, X; \\theta) \\,. $$  \n",
    "\n",
    "When $P(\\mathbf{y} \\,|\\, X; \\theta)$ is considered as a function of the parameters $\\theta = (\\theta_0, \\theta_1, \\ldots, \\theta_n)^T$, then we give this function a special name, the **likelihood function** ( see [here](https://en.wikipedia.org/wiki/Likelihood_function) ).  The notation we use is\n",
    "\n",
    "$$ L(\\theta) = P(\\mathbf{y} \\,|\\, X; \\theta) \\,. $$\n",
    "\n",
    "We want to find $\\theta_0, \\theta_1, \\ldots, \\theta_n$ that maximize the likelihood function $L(\\theta)$.  This is how we will derive the supervised learning algorithm for logistic regression.\n",
    "\n",
    "But let's take a break from this and describe an example we will work through.  This example comes from one of the programming exercises in Andrew Ng's machine learning course.  An administrator of a university department wants to determine each applicant's chance of admission based on their results from two exams.  The administrator has [data](https://github.com/marty-vanhoof/Maching_Learning/blob/master/data/logReg_data1.txt) consisting of previous applicant's exam scores and whether they were admitted to the university of not.  Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam1</th>\n",
       "      <th>exam2</th>\n",
       "      <th>admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       exam1      exam2  admitted\n",
       "0  34.623660  78.024693         0\n",
       "1  30.286711  43.894998         0\n",
       "2  35.847409  72.902198         0\n",
       "3  60.182599  86.308552         1\n",
       "4  79.032736  75.344376         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataframe and display first 5 rows\n",
    "filepath = os.getcwd() + '/logReg_data1.txt'\n",
    "df = pd.read_csv(filepath, names = ['exam1', 'exam2', 'admitted'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data has two features:  ```exam1``` and ```exam2```, and these features are both continuous variables.   The target variable is called ```admitted```, and this is a categorical variable consisting of the labels 0 or 1 (0 means that the applicant was not admitted and 1 means that the applicant was admitted).  We will use this data to build a model that estimates the probability that an applicant will be admitted to the university, given their scores on the two exams.\n",
    "\n",
    "### Boxplot\n",
    "\n",
    "Boxplots give a graphical display of five important statistics associated with any distribution:  the minimum, 25th percentile, median, 75th percentile, and maximum.  They can be very useful for comparing the distributions of continuous variables across different categories.  In this case, we are comparing the distributions of scores for ```exam1``` and ```exam2``` based on whether the applicant was admitted or not.  \n",
    "\n",
    "The boxplots below show that the distributions of exam scores for applicants who were admitted/not admitted are skewed relative to each other.  We can see that applicants who were admitted to the university have higher exam scores (on average) than those who were not admitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFBCAYAAACb7b3CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4CVJapABL0lSg/4/9FmCSNP1lS8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ad4f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split the data into 'admitted_yes', 'admitted_no'\n",
    "admitted_yes = df[ df.admitted == 1 ]\n",
    "admitted_no = df[ df.admitted == 0 ]\n",
    "data = [admitted_yes, admitted_no]\n",
    "xlabels = ['admitted_yes', 'admitted_no']\n",
    "\n",
    "# make the plot\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (8, 5), sharey = True)\n",
    "fig.subplots_adjust(wspace = 0)\n",
    "for i in range(0,2):\n",
    "    ax[i].boxplot( [data[i].exam1, data[i].exam2], widths = [0.4, 0.4] )\n",
    "    ax[i].set( xticklabels = ['exam1', 'exam2'], xlabel = xlabels[i], ylabel = 'score' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scatterplot\n",
    "\n",
    "The scatterplot below seems to suggest that if some function of the exam scores is above a certain threshold, then the applicant will be admitted.  We would like to trace out a boundary between the two classes that will classify the data into 'admitted_yes' or 'admitted_no'.  This boundary is called a **decision boundary**, and logistic regression will help us find the decision boundary and make class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFACAYAAAD07atFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112ecf390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "\n",
    "plt.scatter(admitted_yes.exam1, admitted_yes.exam2, color = 'blue', label = 'admitted_yes')\n",
    "plt.scatter(admitted_no.exam1, admitted_no.exam2, color = 'red', label = 'admitted_no')\n",
    "plt.xlabel('exam1')\n",
    "plt.ylabel('exam2')\n",
    "plt.legend(bbox_to_anchor = (1,0.85), loc = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Cost Function for Logistic Regression\n",
    "\n",
    "Our goal is to find an algorithm that will learn the parameters $\\theta = (\\theta_0, \\theta_1, \\ldots, \\theta_n)^T$ in order to maximize the likelihood function\n",
    "\n",
    "$$ L(\\theta) = P(\\mathbf{y} \\,|\\, X; \\theta) $$\n",
    "\n",
    "Let's assume that our $m$ training examples are *independent*, so that\n",
    "\n",
    "\\begin{align*} \n",
    "P(\\mathbf{y} \\,|\\, X; \\theta) &= \\prod_{i=1}^m P(y_i \\,|\\, \\mathbf{x}_i; \\theta) \\\\\n",
    "                              &= \\prod_{i=1}^m h_\\theta(\\mathbf{x}_i)^{y_i} (1 - h_\\theta(\\mathbf{x}_i))^{1 - y_i} \n",
    "\\end{align*}\n",
    "\n",
    "where the first equality is because of the independence assumption and the last equality is due to $(1)$ above.  In practice, it is easier to maximize the **log-likelihood** $\\ell(\\theta)$, which is the natural logarithm of the likelihood function\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) &= \\ln L(\\theta) \\\\\n",
    "&= \\ln \\bigg[ \\prod_{i=1}^m h_\\theta(\\mathbf{x}_i)^{y_i} (1 - h_\\theta(\\mathbf{x}_i))^{1 - y_i} \\bigg] \\\\ \n",
    "&= \\sum_{i=1}^m y_i \\ln h_\\theta(\\mathbf{x}_i) + (1 - y_i) \\ln(1 - h_\\theta(\\mathbf{x}_i)) \\,. \n",
    "\\end{align*}\n",
    "\n",
    "Since the logarithm function is increasing, maximizing $L(\\theta)$ is equivalent to maximizing $\\ell(\\theta) = \\ln L(\\theta)$.  Also note that maximizing $\\ell(\\theta)$ is equivalent to minimizing $- \\ell(\\theta)$.  The reason we want to minimize $- \\ell(\\theta)$ is because we can use the gradient descent algorithm again.  Recall that in [linear regression](https://github.com/marty-vanhoof/Maching_Learning/blob/master/Linear_Regression/Linear_Regression.ipynb), we used gradient descent to minimize a certain cost function $J(\\theta)$.  In the case of logistic regression, we use the negative log-likelihood as our cost function, so we want to minimize\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m - y_i \\ln h_\\theta(\\mathbf{x}_i) - (1 - y_i) \\ln (1 - h_\\theta(\\mathbf{x}_i)) \\,. $$ \n",
    "\n",
    "It turns out that this function is convex (here is a [proof](http://mathgotchas.blogspot.ca/2011/10/why-is-error-function-minimized-in.html)), so the gradient descent algorithm will eventually converge to the global minimum of $J(\\theta)$ with a suitable learning rate $\\alpha$.  Let's write this cost function in Python using numpy matrix operations.\n",
    "\n",
    "Recall that $h_\\theta(\\mathbf{x}_i) = g(\\theta^T \\mathbf{x}_i)$, where $g$ is the logistic function.  Let $\\mathbf{1}, \\mathbf{ln}_h$ and $\\mathbf{ln}_{1-h}$ be the (m x 1) column vectors defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{1} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{ln}_h =\n",
    "\\begin{bmatrix}\n",
    "\\ln h_\\theta(\\mathbf{x}_1) \\\\\n",
    "\\ln h_\\theta(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\ \n",
    "\\ln h_\\theta(\\mathbf{x}_m)\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{ln}_{1-h} =\n",
    "\\begin{bmatrix}\n",
    "\\ln (1 - h_\\theta(\\mathbf{x}_1)) \\\\\n",
    "\\ln (1 - h_\\theta(\\mathbf{x}_2)) \\\\\n",
    "\\vdots \\\\ \n",
    "\\ln (1 - h_\\theta(\\mathbf{x}_m)) \n",
    "\\end{bmatrix} \\,. \n",
    "$$\n",
    "\n",
    "Then in matrix form, the cost function can be written as\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\big[\\, \\mathbf{y}^T \\mathbf{ln}_h + (\\mathbf{1} - \\mathbf{y})^T \\mathbf{ln}_{1 - h} \\,\\big] \\,. $$\n",
    "\n",
    "We can implement this very easily with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logReg_cost(X, y, theta):\n",
    "    '''Compute the cost function. \n",
    "    Inputs:\n",
    "    X is an m by (n+1) numpy array, where m = # of training examples\n",
    "                                     n = # of features (excluding the \"1\" column)\n",
    "    y is an m by 1 numpy array\n",
    "    theta is a 1 by (n+1) numpy array'''\n",
    "    \n",
    "    # first transform the inputs to numpy matrices\n",
    "    X, y, theta = np.matrix(X), np.matrix(y), np.matrix(theta)\n",
    "    \n",
    "    ln_h = np.log( logistic(X*theta.T) )\n",
    "    ln_1h = np.log( 1 - logistic(X*theta.T) ) \n",
    "    J_theta = ( -1/len(y) )*( y.T*ln_h + (1 - y.T)*ln_1h )\n",
    "    \n",
    "    return J_theta[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in our example, the training data consists of exam scores (```exam1```, ```exam2```) for students who applied to a university, along with a column called ```admitted``` that contains information about whether each student was admitted to the university or not.  This column consists of the labels 1 or 0 (1 if the students was admitted, 0 otherwise). \n",
    "\n",
    "Let's compute the cost function on our dataset with $\\theta$ initialized to all zeros:  $\\, \\theta = (0,0,0)$.  The feature matrix $X$ will consist of the two columns ```exam1``` and ```exam2``` (plus a column of ones by convention).  The target vector $\\mathbf{y}$ will consist of the class labels for each training example, so $\\mathbf{y}$ is just the ```admitted``` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ones</th>\n",
       "      <th>exam1</th>\n",
       "      <th>exam2</th>\n",
       "      <th>admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ones      exam1      exam2  admitted\n",
       "0     1  34.623660  78.024693         0\n",
       "1     1  30.286711  43.894998         0\n",
       "2     1  35.847409  72.902198         0\n",
       "3     1  60.182599  86.308552         1\n",
       "4     1  79.032736  75.344376         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert column of ones and look a first 5 rows\n",
    "df.insert(0, 'ones', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost with initial theta:  0.69314718056\n"
     ]
    }
   ],
   "source": [
    "# get the training data X, target variable y, and initialize theta to (0,0,0)\n",
    "X = df.iloc[:, 0:3].values\n",
    "y = df.iloc[:, 3:4].values\n",
    "theta = np.array([0,0,0]).reshape((1,3))\n",
    "\n",
    "# compute the cost function with initial theta\n",
    "print('cost with initial theta: ', logReg_cost(X, y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "We are trying to minimize the cost function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m - y_i \\ln h_\\theta(\\mathbf{x}_i) - (1 - y_i) \\ln (1 - h_\\theta(\\mathbf{x}_i)) \\,. $$ \n",
    "\n",
    "Since $J(\\theta)$ is a convex function with a global minimum, we can use gradient descent to find this minimum value.  The graph of $J(\\theta)$ is a 3-dimensional manifold in $\\mathbb{R}^4$.  If we start at any point on the manifold, then the negative gradient points in the direction of steepest descent.  The gradient descent algorithm will follow the negative gradient on each iteration, and given a suitable learning rate $\\alpha$ (which controls the size of each step), the algorithm will eventually converge to this global minimum point.\n",
    "\n",
    "Recall from multivariable calculus that the gradient, $\\nabla f$, of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is computed as the matrix of partial derivatives\n",
    "\n",
    "$$\n",
    "\\nabla f =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix} \\,.\n",
    "$$\n",
    "\n",
    "Each iteration of the gradient descent algorithm will update the parameter vector $\\theta$ according to the rule\n",
    "\n",
    "$$ \\theta := \\theta - \\alpha \\nabla J(\\theta) \\,, $$\n",
    "\n",
    "So for $j = 0, \\ldots, n$, each coordinate of $\\theta$ will be updated as\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\, \\qquad (2) $$\n",
    "\n",
    "Next we will compute the partial derivative of $J(\\theta)$ with respect to $\\theta_j$.  As a first step, it's not hard to verify that the derivative of the logistic function is given as follows\n",
    "\n",
    "$$ g'(z) = g(z)(1 - g(z)) \\,. $$\n",
    "\n",
    "(for a derivation, see [Andrew Ng's lecture notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf), Section 5).  So if $z = \\theta_0 + \\theta_1 x_{i1} + \\ldots + \\theta_n x_{in} = \\theta^T \\mathbf{x}_i$, then by the chain rule we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j} g(\\theta^T \\mathbf{x}_i) &= g(\\theta^T \\mathbf{x}_i)(1 - g(\\theta^T \\mathbf{x}_i)) \n",
    "\\frac{\\partial}{\\partial \\theta_j} \\theta^T \\mathbf{x}_i \\\\ \n",
    "                                                            &= g(\\theta^T \\mathbf{x}_i)(1 - g(\\theta^T \\mathbf{x}_i)) \\, x_{ij}  \n",
    "\\end{align*}\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\bigg[ \\frac{1}{m} \\sum_{i=1}^m - y_i \\ln g(\\theta^T \\mathbf{x}_i) - (1 - y_i) \\ln (1 - g(\\theta^T \\mathbf{x}_i)) \\bigg] \\\\\n",
    "                                             &= -\\frac{1}{m} \\sum_{i=1}^m \\bigg[ \\frac{y_i}{g(\\theta^T \\mathbf{x}_i)} - \\frac{1 - y_i}{1 - g(\\theta^T \\mathbf{x}_i)} \\bigg] \\frac{\\partial}{\\partial \\theta_j} g(\\theta^T \\mathbf{x}_i) \\\\\n",
    "                                             &= -\\frac{1}{m} \\sum_{i=1}^m \\bigg[ \\frac{y_i}{g(\\theta^T \\mathbf{x}_i)} - \\frac{1 - y_i}{1 - g(\\theta^T \\mathbf{x}_i)} \\bigg] g(\\theta^T \\mathbf{x}_i)(1 - g(\\theta^T \\mathbf{x}_i)) \\, x_{ij} \\\\\n",
    "                                             &= -\\frac{1}{m} \\sum_{i=1}^m \\big( y_i (1 - g(\\theta^T \\mathbf{x}_i)) - (1 - y_i) g(\\theta^T \\mathbf{x}_i) \\big) \\, x_{ij} \\\\\n",
    "                                             &= -\\frac{1}{m} \\sum_{i=1}^m (y_i - g(\\theta^T \\mathbf{x}_i)) \\, x_{ij}\\\\\n",
    "                                             &= \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) \\, x_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "so the update rule $(2)$ becomes\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) \\, x_{ij} \\quad \\textrm{for all } \\, j = 0, \\ldots, n $$\n",
    "\n",
    "and we should repeat this until the algorithm coverges.  Let's now implement the gradient computation\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) \\, x_{ij} \\quad \\textrm{for all } \\, j = 0, \\ldots, n \\quad (3) $$\n",
    "\n",
    "in Python.  The gradient, $\\nabla J(\\theta)$, is the matrix of partial derivatives from $(3)$, so we can use numpy matrix operations to compute $\\nabla J(\\theta)$.  Let $\\mathbf{h}_\\theta$ be the column vector consisting of the entries $h_\\theta(\\mathbf{x}_i)$ for $i = 1, \\ldots, m$\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_\\theta =\n",
    "\\begin{bmatrix}\n",
    "h_\\theta(\\mathbf{x}_1) \\\\\n",
    "h_\\theta(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta(\\mathbf{x}_m)\n",
    "\\end{bmatrix} \\,. \n",
    "$$\n",
    "\n",
    "The gradient can now be written as\n",
    "\n",
    "$$ \\nabla J(\\theta) = (\\, \\mathbf{h}_\\theta - \\mathbf{y} \\,)^T X $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logReg_gradient(X, y, theta):\n",
    "    '''Compute the gradient of the logistic regression cost function. \n",
    "    Inputs:\n",
    "    X is an m by (n+1) numpy array, where m = # of training examples\n",
    "                                     n = # of features (excluding the \"1\" column)\n",
    "    y is an m by 1 numpy array\n",
    "    theta is a 1 by (n+1) numpy array\n",
    "    Output:\n",
    "    1 by n numpy array of partial derivatives'''\n",
    "    \n",
    "    # transform the inputs to numpy matrices\n",
    "    X, y, theta = np.matrix(X), np.matrix(y), np.matrix(theta)\n",
    "    \n",
    "    h_theta = logistic(X*theta.T)\n",
    "    gradient = (h_theta - y).T * X\n",
    "    return np.array(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -10.         -1200.92165893 -1126.28422055]]\n"
     ]
    }
   ],
   "source": [
    "print( logReg_gradient(X, y, theta) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
