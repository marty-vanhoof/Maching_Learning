{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We can think of linear regression as the \"grandfather\" of supervised machine learning.  It developed as an important technique in statistics long before machine learning existed as a field in its own right.  The idea in supervised learning is that we have a data set called the **training data**.  The training data consists of a set of pairs $\\{ \\, ( \\mathbf{x}_1,y_1 ), ( \\mathbf{x}_2,y_2 ) \\ldots,  ( \\mathbf{x}_m,y_m ) \\, \\}$, where each input variable $\\mathbf{x}_i$ is a vector called a **feature vector**, and the output variables $y_i$ (usually real numbers) are the corresponding **labels**.  \n",
    "\n",
    "If $\\mathcal{X}$ is the input space and $\\mathcal{Y}$ is the output space, then the objective of a supervised learning algorithm is to \"learn\" a function $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that approximates the training data and will generalize in order to predict the output for new instances.  When the target space $\\mathcal{Y}$ consists of continuous data, then the learning problem is called a **regression** problem.  When the target space $\\mathcal{Y}$ consists of discrete data, then the learning problem is called a **classification** problem.\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we have $\\mathcal{X} = \\mathcal{Y} = \\mathbb{R}$, so that $h: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a function from the real numbers to the real numbers, and $h$ is a linear function of the form $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$, where $\\theta_1, \\theta_2$ are called the **weights**.  The training data will then consist of a bunch of points in the plane, and we are trying the find a line that \"best fits\" the data.  This line is called the **least squares line**  (image from [wikipedia](https://en.wikipedia.org/wiki/Linear_regression))\n",
    "\n",
    "<br/>\n",
    "<img src=\"least_squares_line.png\">\n",
    "<br/>\n",
    "\n",
    "Now let's look at an example.  This example comes from one of the programming exercises in Andrew Ng's machine learning course.  Suppose you are the CEO of a food truck franchise and you are considering different cities where you can expand your business.  Your chain already has trucks in a number of cities and you have [data](https://github.com/marty-vanhoof/Maching_Learning/blob/master/data/ex1data1.txt) about the population of each city and the profit of the corresponding food truck.  You would like to estimate the expected profit of a new food truck given only the population of a city (admittedly this example is a little simplistic, but it's good for illustrating the main ideas in linear regression).\n",
    "\n",
    "We will use Pandas to analyze the data.  First we import the modules that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's load the dataframe and take a look at the first 5 rows.  The population and profit columns are in units of $10000, so to get the actual numbers just multiply by 10000.  We can also use Panda's ```describe()``` function to get some statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   population   profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = os.getcwd() + '/ex1data1.txt'\n",
    "df = pd.read_csv(filepath, names = ['population', 'profit'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.0000</td>\n",
       "      <td>97.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.1598</td>\n",
       "      <td>5.8391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.8699</td>\n",
       "      <td>5.5103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.0269</td>\n",
       "      <td>-2.6807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.7077</td>\n",
       "      <td>1.9869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.5894</td>\n",
       "      <td>4.5623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.5781</td>\n",
       "      <td>7.0467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.2030</td>\n",
       "      <td>24.1470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       population   profit\n",
       "count     97.0000  97.0000\n",
       "mean       8.1598   5.8391\n",
       "std        3.8699   5.5103\n",
       "min        5.0269  -2.6807\n",
       "25%        5.7077   1.9869\n",
       "50%        6.5894   4.5623\n",
       "75%        8.5781   7.0467\n",
       "max       22.2030  24.1470"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some basic statistics\n",
    "df.describe().apply(lambda x: x.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatterplot of the data suggests a positive correlation between population and profit, and many of the data points cluster around cities with lower populations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10eb619b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatterplot of the data\n",
    "plt.scatter(df.population, df.profit)\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('profit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Gradient Descent Algorithm\n",
    "\n",
    "Let's write the feature vector $\\mathbf{x}_i$ as a row vector $\\mathbf{x}_i = (x_{i0}, x_{i1}, \\dots, x_{in})$, where $x_{i0} = 1$ for all $i = 1, \\ldots, m$, so that $n$ is the dimension of the feature space $\\mathcal{X}$ and $m$ is the number of training examples.  We want an algorithm that will find the parameters $\\theta_0, \\theta_1, \\ldots, \\theta_n$ so that the function $h_\\theta(\\mathbf{x}_i) = \\theta_0 x_{i0} + \\theta_2 x_{i1} + \\ldots + \\theta_n x_{in}$  provides an optimal fit to the data.  The algorithm that does this is called $\\textbf{gradient} \\, \\textbf{descent}$.  By convention, we always set the first component of the vector $\\mathbf{x}_i$ equal to 1, because we need to have the constant term $\\theta_0$ in the expression for $h_\\theta(\\mathbf{x}_i)$.\n",
    "\n",
    "The gradient descent algorithm will find the parameters $\\theta = (\\theta_0, \\theta_1 \\ldots, \\theta_n)$ that minimize the function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m ( h_{\\theta}( \\mathbf{x}_i ) - y_i )^2 \\, , $$\n",
    "\n",
    "where $$ h_{\\theta}(\\mathbf{x}_i) = \\theta_0 x_{i0} + \\theta_2 x_{i2} + \\ldots + \\theta_n x_{in} = \\sum_{j=0}^n \\theta_j x_{ij} $$\n",
    "\n",
    "This function $J(\\theta)$ is called the **residual sum of squares** (RSS) in statistics, and in the machine learning literature it is often called a $\\textbf{cost} \\, \\mathbf{function}$.  It is an average of the squared distances between each $h_{\\theta}(\\mathbf{x}_i)$ and the target variable $y_i$, and it measures how tightly our function $h_\\theta$ fits the data.  \n",
    "\n",
    "Let's implement this cost function in Python using numpy's linear algebra capabilities.  In order to do this, we should write $J(\\theta)$ in matrix form.  If we put each feature vector $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_m$ as the rows of a matrix $X$ and the labels $y_1, y_2, \\ldots, y_m$ in a column vector $\\mathbf{y}$, then we have \n",
    "\n",
    "<br/>\n",
    "$$ X = \n",
    "\\begin{bmatrix} \\tag{1}\n",
    "1 & x_{12} & x_{13} & \\cdots & x_{1n} \\\\\n",
    "1 & x_{22} & x_{23} & \\cdots & x_{2n} \\\\\n",
    "\\vdots & \\vdots & & & \\vdots \\\\\n",
    "1 & x_{m2} & x_{m3} & \\cdots & x_{mn}\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{bmatrix} \\, , \\quad\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 & \\theta_2 & \\cdots & \\theta_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "and if $\\theta^T$ is the transpose of $\\theta$, then we can write\n",
    "\n",
    "<br/>\n",
    "$$ X \\theta^T - \\mathbf{y} = \n",
    "\\begin{bmatrix} \\tag{2}\n",
    "h_\\theta(\\mathbf{x}_1) - y_1 \\\\\n",
    "h_\\theta(\\mathbf{x}_2) - y_2 \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta(\\mathbf{x}_m) - y_m\n",
    "\\end{bmatrix} \\, .\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "Therefore, if we square each entry of $X \\theta^T - \\mathbf{y}$, then add all the entries and divide by $2m$, we get the expression for $J(\\theta)$.  Numpy can do this very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    '''Compute the cost function. \n",
    "    Inputs:\n",
    "    X is an m by (n+1) matrix, where m = # of training examples\n",
    "                                     n = # of features (excluding the \"1\" column)\n",
    "    y is an m-dimensional vector (array)\n",
    "    theta is an n-dimensional vector (array)'''\n",
    "    \n",
    "    squared_residuals = np.power(X*theta.T - y, 2)\n",
    "    return np.sum(squared_residuals) / (2 * len(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm starts with some initial value of $\\theta$, say $\\theta = (0, 0, \\ldots, 0)$, and then updates the components of $\\theta$ on each iteration in order to reduce $J(\\theta)$ until we get to the minimum of $J(\\theta)$.  The update rule is given by\n",
    "\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\quad \\textrm{for all } \\, j = 0, \\ldots, n $$\n",
    "\n",
    "where $\\alpha$ is a parameter called the $\\textbf{learning} \\, \\textbf{rate}$.  We use the symbol \":=\" for the assignment operator, so that in each iteration, all the values of $\\theta_j$ are overwritten by the values on the right-hand side.  \n",
    "\n",
    "If you know some calculus, then it's not hard to verify that \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) \\, x_{ij} \\quad \\textrm{for all } \\, j = 0, \\ldots, n $$\n",
    "\n",
    "so the update rule becomes\n",
    "\n",
    "\\begin{equation} \\tag{3}\n",
    "\\theta_j := \\theta_j -  \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(\\mathbf{x}_i) - y_i) \\, x_{ij} \\quad \\textrm{( for all j )} \n",
    "\\end{equation}\n",
    "\n",
    "and we should repeat until the algorithm converges.  This begs the question of whether the algorithm will always converge, and the answer is yes, at least for linear regression and when the learning rate $\\alpha$ is not too large. \n",
    "\n",
    "There is a very geometric reason why this algorithm works.  When $h_\\theta(\\mathbf{x}_i)$ is linear, the function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m ( h_{\\theta}( \\mathbf{x}_i ) - y_i )^2 \\, , $$\n",
    "\n",
    "is a convex quadratic function in the variables $\\theta_0, \\theta_1, \\ldots, \\theta_n$, so the graph of $J(\\theta)$ is a smooth $n$-dimensional \"surface\" (called a manifold) that is convex and has a global minimum.  The [gradient](https://en.wikipedia.org/wiki/Gradient) is the vector field that gives the direction of steepest ascent at each point on the manifold.  Then the negative of the gradient will give the direction of steepest descent at every point.  This is where the name \"gradient descent\" comes from.  So if we start at a particular point on the manifold, then the algorithm tells us to follow the negative gradient on each iteration, and this will eventually lead us to the global minimum.  The cost function will decrease on every iteration, but the magnitude of the changes become smaller and smaller as we approach the global minimum.  In practice, we usually stop the algorithm before the global minimum is reached.  To visualize it better, think of a 2 dimensional surface that is bowl-shaped, such as the one on [this page](http://charlesfranzen.com/posts/multiple-regression-in-python-gradient-descent/).\n",
    "\n",
    "This algorithm we just described is actually called **batch gradient descent** and generally works well except when the number of training examples is really large.  The batch gradient descent algorithm will read in and sum over all the training examples at every iteration.  This is computationally expensive when the dataset is really large.  In this case, there's an alternative algorithm called [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), but we won't get into that.\n",
    "\n",
    "Let's go back to the example about the food truck franchise.  Recall that the first 5 rows of our dataset look like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   population   profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first compute the cost function $J(\\theta)$ for this dataset, which means that we need the values for $X, \\mathbf{y}$, and $\\theta$.  We will initialize each component of $\\theta$ to zero:  $\\theta = (0,0)$.  The vector $\\mathbf{y}$ is the profit column, and our matrix $X$ will be the other column plus a column of ones that we need to insert into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ones</th>\n",
       "      <th>population</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ones  population   profit\n",
       "0     1      6.1101  17.5920\n",
       "1     1      5.5277   9.1302\n",
       "2     1      8.5186  13.6620\n",
       "3     1      7.0032  11.8540\n",
       "4     1      5.8598   6.8233"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert column of ones\n",
    "df.insert(0, 'ones', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost with initial theta:  32.0727338775\n"
     ]
    }
   ],
   "source": [
    "# get the training data X and the target variable y\n",
    "X = df[['ones', 'population']]\n",
    "y = df['profit']\n",
    "\n",
    "# initialize theta to (0,0)\n",
    "theta = np.array([0,0])\n",
    "\n",
    "# transform into numpy matrices so we can do linear algebra with them\n",
    "X, y, theta = np.matrix(X), np.matrix(y).T, np.matrix(theta)\n",
    "\n",
    "# compute the cost function with initial theta = (0,0)\n",
    "print( 'Cost with initial theta: ', compute_cost(X, y, theta) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient Descent\n",
    "\n",
    "Let's now implement the batch gradient descent algorithm using the update rule $\\mathrm{(3)}$ from above.  To do this compactly, we will write the update rule in matrix form and then exploit numpy's linear algebra capabilities.  Using the matrices $X$ and $X \\theta^T - \\mathbf{y}$ from $\\mathrm{(1)}$ and $\\mathrm{(2)}$ above and doing a little matrix algebra, we can show that the update rule $\\mathrm{(3)}$ can be written as\n",
    "\n",
    "$$ \\theta := \\theta - \\frac{\\alpha}{m} (X \\theta^T - \\mathbf{y})^T X \\,\\,, $$\n",
    "\n",
    "where $(X \\theta^T - \\mathbf{y})^T$ is the transpose of the matrix $X \\theta^T - \\mathbf{y}$.  We can use this to update all the parameters $\\theta = (\\theta_0, \\ldots, \\theta_n)$ in each iteration without resorting to nested \"for\" loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted theta parameters:  [[-3.24140214  1.1272942 ]]\n",
      "cost:  4.51595550308\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    '''Implement the gradient descent algorithm.\n",
    "    Inputs:\n",
    "    X is an m by (n+1) matrix, where m = # of training examples\n",
    "                                     n = # of features (excluding the \"1\" column)\n",
    "    y is an m-dimensional vector (array)\n",
    "    theta is an n-dimensional vector (array)\n",
    "    alpha is the learning rate\n",
    "    num_iterations is how many times we want to run the algorithm'''\n",
    "    \n",
    "    m = len(y)\n",
    "    # to store the values of J(theta) on each iteration\n",
    "    cost_history = np.empty(num_iterations)\n",
    "    \n",
    "    for k in range(0, num_iterations):\n",
    "        residuals = X*theta.T - y\n",
    "        # update rule \n",
    "        theta = theta - (alpha / m) * (residuals.T)*X\n",
    "        cost_history[k] = compute_cost(X, y, theta)\n",
    "        \n",
    "    return theta, cost_history\n",
    "        \n",
    "# run the algorithm with the parameters below\n",
    "alpha = 0.01\n",
    "num_iterations= 1000\n",
    "\n",
    "theta_fit, cost_hist = gradient_descent(X, y, theta, alpha, num_iterations)\n",
    "print('fitted theta parameters: ', theta_fit)\n",
    "print('cost: ', cost_hist[999] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
